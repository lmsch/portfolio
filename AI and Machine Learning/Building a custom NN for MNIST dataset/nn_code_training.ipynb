{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q6NeZ9hn3gsI"
   },
   "source": [
    "# Assignment 3\n",
    "Luke Schipper  \n",
    "CS4210 - Machine Learning  \n",
    "10 April 2021  \n",
    "\n",
    "My goal for this assignment was to take the NN function from our lectures and make it scalable. The NN function should be able to take in any value for number of layers, hidden units, and outputs. Furthermore, regularization and different activation functions must be supported. To do this, I\n",
    "\n",
    "* converted forward and back propagation to use any number of hidden layers, rather than just a single hidden layer. \n",
    "* changed the last layer L from sigmoid to softmax. L - 1 layers can use either tanh, relu, or sigmoid. \n",
    "* modified the cost function to use categorical cross-entropy rather than binary cross-entropy\n",
    "* added L2 regularization with weight decay in back propagation\n",
    "\n",
    "Upon completing these tasks, I trained various models in order to maximize accuracy on the MNIST test set. Therefore, no cross-validation set was used to tune parameters. The model that, after training, demonstrated the highest accuracy on the test set was selected.\n",
    "\n",
    "The highest accuracy achieved was 98.13%. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "isY9debrWW2I"
   },
   "outputs": [],
   "source": [
    "!pip install tensorflow\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time # To measure training time.\n",
    "from tensorflow.keras import datasets, utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "_z1p1s7tWW2h"
   },
   "outputs": [],
   "source": [
    "# This function initializes the weights and biases with small random values. It is mostly\n",
    "# the same as the original code, but has been converted to use a variable number of hidden\n",
    "# layers.\n",
    "def init_params(n_x, n_hl, n_hu, n_y):\n",
    "    params = {}\n",
    "    np.random.seed(2)\n",
    "    params['W1'] = np.random.randn(n_hu, n_x) * 0.01\n",
    "    params['b1'] = np.zeros((n_hu, 1))\n",
    "    if n_hl > 1:\n",
    "      for i in range(2, n_hl + 1): \n",
    "        params['W' + str(i)] = np.random.randn(n_hu, n_hu) * 0.01\n",
    "        params['b' + str(i)] = np.zeros((n_hu, 1))\n",
    "    params['W' + str(n_hl + 1)] = np.random.randn(n_y, n_hu) * 0.01\n",
    "    params['b' + str(n_hl + 1)] = np.zeros((n_y, 1))\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "lyiw6PqdWW2n"
   },
   "outputs": [],
   "source": [
    "# The activation functions that can be used for all layers except the last.\n",
    "# activations['sigmoid'][0] is the function itself (for forward prop)\n",
    "# activations['sigmoid'][1] is the derivative (for back prop)\n",
    "activations = {\n",
    "    'sigmoid': (\n",
    "        lambda x: 1/(1 + np.exp(-x)), \n",
    "        lambda x: 1/(1 + np.exp(-x)) - (1/(1 + np.exp(-x)))**2\n",
    "    ),\n",
    "    'tanh': (\n",
    "        lambda x: np.tanh(x),\n",
    "        lambda x: 1 - np.tanh(x)**2\n",
    "    ),\n",
    "    'relu': (\n",
    "        lambda x: x * (x > 0),\n",
    "        lambda x: 1 * (x > 0)\n",
    "    ),\n",
    "}\n",
    "\n",
    "# The softmax activation for the final layer. Gives a valid probability distributation.\n",
    "# Used http://www.adeveloperdiary.com/data-science/deep-learning/neural-network-with-softmax-in-python/\n",
    "# as a guide for back prop equation derivation and python implementation.\n",
    "def softmax(x):\n",
    "  exps = np.exp(x - np.max(x))\n",
    "  return exps / exps.sum(axis = 0, keepdims = True)\n",
    "\n",
    "# Standard forward propagation adapted to use multiple hidden layers.\n",
    "# Can choose activation used by all layers except the last, which is stictly softmax.\n",
    "def forward_propagation(X, params, n_hl, act_key):\n",
    "    forward = {}\n",
    "    Z1 = np.dot(params['W1'], X) + params['b1']\n",
    "    forward['Z1'] = Z1\n",
    "    forward['A1'] = activations[act_key][0](Z1)\n",
    "    if n_hl > 1:\n",
    "      for i in range(2, n_hl + 1):\n",
    "        Z = np.dot(params['W' + str(i)], forward['A' + str(i - 1)]) + params['b' + str(i)]\n",
    "        forward['Z' + str(i)] = Z\n",
    "        forward['A' + str(i)] = activations[act_key][0](Z)\n",
    "    Zlast = np.dot(params['W' + str(n_hl + 1)], forward['A' + str(n_hl)]) + params['b' + str(n_hl + 1)]\n",
    "    Alast = softmax(Zlast)\n",
    "    forward['Z' + str(n_hl + 1)] = Zlast\n",
    "    forward['A' + str(n_hl + 1)] = Alast\n",
    "    return Alast, forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "6Y_101YQWW2t"
   },
   "outputs": [],
   "source": [
    "# Compute cost to display when training.\n",
    "def compute_cost(Y_pred, Y, params, n_hl, lmda):\n",
    "    m = Y.shape[1]\n",
    "    # Compute average generalized cross-entropy. Add epsilon to avoid np.log(0).\n",
    "    entropy_cost = -(1/m) * np.sum(Y * np.log(Y_pred + 1e-8))\n",
    "    # Compute L2 regularization cost from weights.\n",
    "    reg_cost = 0\n",
    "    for i in range(1, n_hl + 2):\n",
    "      reg_cost += np.sum(params['W' + str(i)]**2)\n",
    "    # Multiply by hyperparameter lambda, average.\n",
    "    reg_cost *= lmda/(2 * m)\n",
    "    return entropy_cost + reg_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "q9VtgD8hWW2y"
   },
   "outputs": [],
   "source": [
    "# Back propagation adapted to use variable hidden layers, activations. Can modify lambda\n",
    "# to increase/reduce regularization effect. \n",
    "# Used https://towardsdatascience.com/deriving-the-backpropagation-equations-from-scratch-part-1-343b300c585a\n",
    "# for generic back propagation equations (for functions other than sigmoid).\n",
    "# Used https://towardsdatascience.com/understanding-the-scaling-of-l%C2%B2-regularization-in-the-context-of-neural-networks-e3d25f8b50db\n",
    "# for weight decay equations.\n",
    "def backward_propagation(X, Y, params, forward, n_hl, lmda, act_key):\n",
    "    grads = {}\n",
    "    m = X.shape[1]\n",
    "    dZlast = forward['A' + str(n_hl + 1)] - Y\n",
    "    grads['dZ' + str(n_hl + 1)] = dZlast\n",
    "    # Generic gradient calculations with weight decay added at the end.\n",
    "    grads['dW' + str(n_hl + 1)] = (1/m) * np.dot(dZlast, forward['A' + str(n_hl)].T) + (lmda/m) * params['W' + str(n_hl + 1)]\n",
    "    grads['db' + str(n_hl + 1)] = (1/m) * np.sum(dZlast, axis=1, keepdims=True)\n",
    "    if n_hl > 1:\n",
    "      for i in range(n_hl, 1, -1):\n",
    "        dZ = np.multiply(np.dot(params['W' + str(i + 1)].T, grads['dZ' + str(i + 1)]), activations[act_key][1](forward['Z' + str(i)]))\n",
    "        grads['dZ' + str(i)] = dZ\n",
    "        grads['dW' + str(i)] = (1/m) * np.dot(dZ, forward['A' + str(i - 1)].T) + (lmda/m) * params['W' + str(i)]\n",
    "        grads['db' + str(i)] = (1/m) * np.sum(dZ, axis=1, keepdims=True)\n",
    "    dZ1 = np.multiply(np.dot(params['W2'].T, grads['dZ2']), activations[act_key][1](forward['Z1']))\n",
    "    grads['dZ1'] = dZ1\n",
    "    grads['dW1'] = (1/m) * np.dot(dZ1, X.T) + (lmda/m) * params['W1']\n",
    "    grads['db1'] = (1/m) * np.sum(dZ1, axis=1, keepdims=True)\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "i0tteT9VWW3E"
   },
   "outputs": [],
   "source": [
    "# Helper function to update weights with gradients at the end of back propagation.\n",
    "# Same as before but adapted for multiple hidden layers.\n",
    "def update_params(params, grads, n_hl, learn_rate):\n",
    "    for i in range(1, n_hl + 2):\n",
    "      params['W' + str(i)] = params['W' + str(i)] - learn_rate * grads['dW' + str(i)]\n",
    "      params['b' + str(i)] = params['b' + str(i)] - learn_rate * grads['db' + str(i)]\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "P0_yKzDzWW3H"
   },
   "outputs": [],
   "source": [
    "# The original NN funciton.\n",
    "# n_hl = number of hidden layers\n",
    "# n_hu = number of hidden units\n",
    "# act_key = function to use for L-1 layers (sigmoid, relu, tanh)\n",
    "def nn_model(X, Y, n_hl, n_hu, lmda, act_key, num_iterations, learn_rate, print_cost = True, print_freq = 0):\n",
    "    n_x = X.shape[0]\n",
    "    n_y = Y.shape[0]\n",
    "    # by default, print 5 times on each training.\n",
    "    print_freq = print_freq if print_freq > 0 else num_iterations // 5\n",
    "    params = init_params(n_x, n_hl, n_hu, n_y)\n",
    "    cost = 0\n",
    "    for i in range(0, num_iterations):\n",
    "        A, forward = forward_propagation(X, params, n_hl, act_key)\n",
    "        cost = compute_cost(A, Y, params, n_hl, lmda)\n",
    "        grads = backward_propagation(X, Y, params, forward, n_hl, lmda, act_key)\n",
    "        params = update_params(params, grads, n_hl, learn_rate)\n",
    "        if print_cost and i % print_freq == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "    if print_cost:\n",
    "        # print final cost at the end of training\n",
    "        print(\"Final cost: %f\" %(cost))\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "xxRn8nNlWW3J"
   },
   "outputs": [],
   "source": [
    "# Uses NN parameters from training to predict a set of test examples.\n",
    "def predict(X, params, n_hl, act_key):\n",
    "    Y_pred, forward = forward_propagation(X, params, n_hl, act_key)\n",
    "    # The output is the index of the largest probability in each prediction.\n",
    "    # ex. [0.05, 0.05, 0.05, 0.05, 0.6, 0.05, 0.05, 0.05, 0.025, 0.025]\n",
    "    # predicts 4, since its probability is the highest at 60%.\n",
    "    output = np.argmax(Y_pred, axis=0)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "MkpE_2ZnXFrU"
   },
   "outputs": [],
   "source": [
    "# Reports accuracy from NN predictions and the correct labels.\n",
    "def accuracy(Y_pred, Y):\n",
    "    return 100 * (np.sum(Y_pred == Y) / Y.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "ls7UDW5HV1aW"
   },
   "outputs": [],
   "source": [
    "# Taken from the lecture on ConvNets. Reshapes the MNIST dataset to two dimensions.\n",
    "# Normalizes gray level values to be between 0 and 1.\n",
    "# Labels are encoded with one-hot.\n",
    "(train_images, train_labels), (test_images, test_labels) = datasets.mnist.load_data()\n",
    "Xtrain = train_images.reshape(60000, 784).astype('float32').T / 255.0\n",
    "Xtest = test_images.reshape(10000, 784).astype('float32').T / 255.0\n",
    "Ytrain = tf.keras.utils.to_categorical(train_labels, 10).T\n",
    "Ytest = tf.keras.utils.to_categorical(test_labels, 10).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to train models with various combinations of hyperparameter values.\n",
    "# Reports training cost, training time, accuracy, and the best run at the end.\n",
    "def parameter_tuning(layer_list, unit_list, rate_list, lmda_list, act_list, num_iterations):\n",
    "    results = [];\n",
    "    for n_hl in layer_list:\n",
    "        for n_hu in unit_list:\n",
    "            for rate in rate_list:\n",
    "                for lmda in lmda_list:\n",
    "                    for act in act_list:\n",
    "                        current_run = { \n",
    "                            'run': len(results) + 1,\n",
    "                            'n_hl': n_hl,\n",
    "                            'n_hu': n_hu,\n",
    "                            'rate': rate,\n",
    "                            'lmda': lmda,\n",
    "                            'act': act\n",
    "                        }\n",
    "                        print('Run start ------------------------')\n",
    "                        print('\\n'.join(\"{}:\\t{}\".format(k, v) for k, v in current_run.items()))\n",
    "                        start = time.time()\n",
    "                        params = nn_model(Xtrain, Ytrain, n_hl, n_hu, lmda, act, num_iterations, rate, True)\n",
    "                        end = time.time()\n",
    "                        predictions = predict(Xtest, params, n_hl, act)\n",
    "                        acc = accuracy(predictions, test_labels)\n",
    "                        current_run['acc'] = acc\n",
    "                        current_run['params'] = params\n",
    "                        print(f'Accuracy: {acc}%')\n",
    "                        print(f'Train time: {(end - start) // 60} minutes')\n",
    "                        print('Run end --------------------------', end = '\\n\\n')\n",
    "                        results.append(current_run)\n",
    "    best_run = max(results, key = lambda x: x['acc'])\n",
    "    print('Best run -------------------------')\n",
    "    print('\\n'.join(\"{}:\\t{}\".format(k, '{}' if isinstance(v, dict) else v) for k, v in best_run.items()))\n",
    "    print('Run end --------------------------')\n",
    "    return best_run, results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After some initial testing, I found that adding more layers did not improve accuracy with a fixed, low number of training iterations. For the network to converge with more layers added, many more training iterations are needed.\n",
    "\n",
    "Furthermore, increasing the number of hidden units seemed to greatly improve performance. Starting with 4 hidden units on a single hidden layer NN, adding 6 units brought accuracy above 90% after only 1000 iterations. Adding 100 units brought accuracy around ~96%.\n",
    "\n",
    "After some research, I discovered that, in theory, a NN with 2 hidden layers can model any decision boundary, meaning it is not usually beneficial to add more. \n",
    "(see https://stats.stackexchange.com/questions/181/how-to-choose-the-number-of-hidden-layers-and-nodes-in-a-feedforward-neural-netw)\n",
    "\n",
    "Furthermore, the ideal number of hidden units tends to fall between the number of inputs (784) and the number of outputs (10). Therefore, I narrowed my parameter search space to this range.\n",
    "\n",
    "Due to time constraints, I decided to keep num_iterations fairly low for the initial tuning. The goal here was to narrow the search space for number of layers and hidden units, while also choosing the best activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6eWPwU7HjsjW",
    "outputId": "c29f41de-5c26-4c54-84d7-6ff3bccd186f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Run start ------------------------\n",
      "run:\t1\n",
      "n_hl:\t1\n",
      "n_hu:\t268\n",
      "rate:\t1\n",
      "lmda:\t0.0001\n",
      "act:\ttanh\n",
      "Cost after iteration 0: 2.302403\n",
      "Cost after iteration 200: 5.504408\n",
      "Cost after iteration 400: 0.189902\n",
      "Cost after iteration 600: 0.114033\n",
      "Cost after iteration 800: 0.082633\n",
      "Final cost: 0.067865\n",
      "Accuracy: 96.71%\n",
      "Train time: 18.0 minutes\n",
      "Run end --------------------------\n",
      "\n",
      "Run start ------------------------\n",
      "run:\t2\n",
      "n_hl:\t1\n",
      "n_hu:\t268\n",
      "rate:\t1\n",
      "lmda:\t0.0001\n",
      "act:\tsigmoid\n",
      "Cost after iteration 0: 2.305423\n",
      "Cost after iteration 200: 0.348008\n",
      "Cost after iteration 400: 0.279593\n",
      "Cost after iteration 600: 0.244734\n",
      "Cost after iteration 800: 0.216966\n",
      "Final cost: 0.193975\n",
      "Accuracy: 94.31%\n",
      "Train time: 19.0 minutes\n",
      "Run end --------------------------\n",
      "\n",
      "Run start ------------------------\n",
      "run:\t3\n",
      "n_hl:\t1\n",
      "n_hu:\t268\n",
      "rate:\t1\n",
      "lmda:\t0.0001\n",
      "act:\trelu\n",
      "Cost after iteration 0: 2.301998\n",
      "Cost after iteration 200: 0.167412\n",
      "Cost after iteration 400: 0.105562\n",
      "Cost after iteration 600: 0.077154\n",
      "Cost after iteration 800: 0.059944\n",
      "Final cost: 0.048185\n",
      "Accuracy: 97.7%\n",
      "Train time: 10.0 minutes\n",
      "Run end --------------------------\n",
      "\n",
      "Run start ------------------------\n",
      "run:\t4\n",
      "n_hl:\t1\n",
      "n_hu:\t526\n",
      "rate:\t1\n",
      "lmda:\t0.0001\n",
      "act:\ttanh\n",
      "Cost after iteration 0: 2.305100\n",
      "Cost after iteration 200: 1.252372\n",
      "Cost after iteration 400: 0.230354\n",
      "Cost after iteration 600: 0.233161\n",
      "Cost after iteration 800: 1.169617\n",
      "Final cost: 0.080914\n",
      "Accuracy: 96.31%\n",
      "Train time: 34.0 minutes\n",
      "Run end --------------------------\n",
      "\n",
      "Run start ------------------------\n",
      "run:\t5\n",
      "n_hl:\t1\n",
      "n_hu:\t526\n",
      "rate:\t1\n",
      "lmda:\t0.0001\n",
      "act:\tsigmoid\n",
      "Cost after iteration 0: 2.308512\n",
      "Cost after iteration 200: 0.411130\n",
      "Cost after iteration 400: 0.287780\n",
      "Cost after iteration 600: 0.247108\n",
      "Cost after iteration 800: 0.218881\n",
      "Final cost: 0.196573\n",
      "Accuracy: 94.22%\n",
      "Train time: 35.0 minutes\n",
      "Run end --------------------------\n",
      "\n",
      "Run start ------------------------\n",
      "run:\t6\n",
      "n_hl:\t1\n",
      "n_hu:\t526\n",
      "rate:\t1\n",
      "lmda:\t0.0001\n",
      "act:\trelu\n",
      "Cost after iteration 0: 2.304968\n",
      "Cost after iteration 200: 0.176782\n",
      "Cost after iteration 400: 0.107178\n",
      "Cost after iteration 600: 0.077475\n",
      "Cost after iteration 800: 0.059756\n",
      "Final cost: 0.047851\n",
      "Accuracy: 97.68%\n",
      "Train time: 20.0 minutes\n",
      "Run end --------------------------\n",
      "\n",
      "Run start ------------------------\n",
      "run:\t7\n",
      "n_hl:\t2\n",
      "n_hu:\t268\n",
      "rate:\t1\n",
      "lmda:\t0.0001\n",
      "act:\ttanh\n",
      "Cost after iteration 0: 2.303135\n",
      "Cost after iteration 200: 117.350416\n",
      "Cost after iteration 400: 116.938153\n",
      "Cost after iteration 600: 164.434420\n",
      "Cost after iteration 800: 109.129109\n",
      "Final cost: 112.459148\n",
      "Accuracy: 10.32%\n",
      "Train time: 31.0 minutes\n",
      "Run end --------------------------\n",
      "\n",
      "Run start ------------------------\n",
      "run:\t8\n",
      "n_hl:\t2\n",
      "n_hu:\t268\n",
      "rate:\t1\n",
      "lmda:\t0.0001\n",
      "act:\tsigmoid\n",
      "Cost after iteration 0: 2.305141\n",
      "Cost after iteration 200: 1.931395\n",
      "Cost after iteration 400: 1.694563\n",
      "Cost after iteration 600: 1.377300\n",
      "Cost after iteration 800: 0.964362\n",
      "Final cost: 0.509417\n",
      "Accuracy: 85.07000000000001%\n",
      "Train time: 33.0 minutes\n",
      "Run end --------------------------\n",
      "\n",
      "Run start ------------------------\n",
      "run:\t9\n",
      "n_hl:\t2\n",
      "n_hu:\t268\n",
      "rate:\t1\n",
      "lmda:\t0.0001\n",
      "act:\trelu\n",
      "Cost after iteration 0: 2.302619\n",
      "Cost after iteration 200: 0.350990\n",
      "Cost after iteration 400: 0.139732\n",
      "Cost after iteration 600: 0.261364\n",
      "Cost after iteration 800: 0.208641\n",
      "Final cost: 0.158335\n",
      "Accuracy: 94.61%\n",
      "Train time: 17.0 minutes\n",
      "Run end --------------------------\n",
      "\n",
      "Run start ------------------------\n",
      "run:\t10\n",
      "n_hl:\t2\n",
      "n_hu:\t526\n",
      "rate:\t1\n",
      "lmda:\t0.0001\n",
      "act:\ttanh\n",
      "Cost after iteration 0: 2.301854\n",
      "Cost after iteration 200: 255.283934\n",
      "Cost after iteration 400: 238.047936\n",
      "Cost after iteration 600: 235.903644\n",
      "Cost after iteration 800: 248.330481\n",
      "Final cost: 280.829021\n",
      "Accuracy: 9.74%\n",
      "Train time: 59.0 minutes\n",
      "Run end --------------------------\n",
      "\n",
      "Run start ------------------------\n",
      "run:\t11\n",
      "n_hl:\t2\n",
      "n_hu:\t526\n",
      "rate:\t1\n",
      "lmda:\t0.0001\n",
      "act:\tsigmoid\n",
      "Cost after iteration 0: 2.305842\n",
      "Cost after iteration 200: 1.800398\n",
      "Cost after iteration 400: 1.713511\n",
      "Cost after iteration 600: 1.325786\n",
      "Cost after iteration 800: 0.786034\n",
      "Final cost: 0.442112\n",
      "Accuracy: 86.95%\n",
      "Train time: 71.0 minutes\n",
      "Run end --------------------------\n",
      "\n",
      "Run start ------------------------\n",
      "run:\t12\n",
      "n_hl:\t2\n",
      "n_hu:\t526\n",
      "rate:\t1\n",
      "lmda:\t0.0001\n",
      "act:\trelu\n",
      "Cost after iteration 0: 2.302358\n",
      "Cost after iteration 200: 0.333509\n",
      "Cost after iteration 400: 0.114281\n",
      "Cost after iteration 600: 0.065742\n",
      "Cost after iteration 800: 1.468428\n",
      "Final cost: 1.107055\n",
      "Accuracy: 69.47%\n",
      "Train time: 34.0 minutes\n",
      "Run end --------------------------\n",
      "\n",
      "Best run -------------------------\n",
      "run:\t3\n",
      "n_hl:\t1\n",
      "n_hu:\t268\n",
      "rate:\t1\n",
      "lmda:\t0.0001\n",
      "act:\trelu\n",
      "acc:    97.7%\n",
      "params: {}\n",
      "Run end --------------------------\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "num_iterations = 1000\n",
    "layer_list = [1, 2] # Try 1 and 2 layers for reasons mentioned above.\n",
    "unit_list = [268, 526] # Try 1/3rd increments between 10 and 784.\n",
    "rate_list = [1] # Keep fixed.\n",
    "lmda_list = [0.0001] # Keep regularization effect fixed and very low.\n",
    "act_list = ['tanh', 'sigmoid', 'relu'] # Try all activation functions.\n",
    "best_run, results = parameter_tuning(layer_list, unit_list, rate_list, lmda_list, act_list, num_iterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Across the board, n_hl = 1 performed way better than n_hl = 2. RELU performed slightly better than tanh and sigmoid with n_hl = 1, both in accuracy and in training time. Between n_hu = 268 and n_hu = 526, there was a 0.02 difference in accuracy. With more training iterations, n_hu = 526 might do better, but due to time constraints, I favored lower training time.\n",
    "\n",
    "I then try more tuning to find the best number of hidden units, keeping hidden layers fixed at 1. With less combinations of parameters, I increased num_iterations to give the larger networks a better chance to converge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run start ------------------------\n",
      "run:\t1\n",
      "n_hl:\t1\n",
      "n_hu:\t96\n",
      "rate:\t1\n",
      "lmda:\t0.0001\n",
      "act:\trelu\n",
      "Cost after iteration 0: 2.303917\n",
      "Cost after iteration 400: 0.118304\n",
      "Cost after iteration 800: 0.071523\n",
      "Cost after iteration 1200: 0.050105\n",
      "Cost after iteration 1600: 0.037255\n",
      "Final cost: 0.028459\n",
      "Accuracy: 97.71%\n",
      "Train time: 14.0 minutes\n",
      "Run end --------------------------\n",
      "\n",
      "Run start ------------------------\n",
      "run:\t2\n",
      "n_hl:\t1\n",
      "n_hu:\t182\n",
      "rate:\t1\n",
      "lmda:\t0.0001\n",
      "act:\trelu\n",
      "Cost after iteration 0: 2.303443\n",
      "Cost after iteration 400: 0.106396\n",
      "Cost after iteration 800: 0.060236\n",
      "Cost after iteration 1200: 0.039389\n",
      "Cost after iteration 1600: 0.027538\n",
      "Final cost: 0.020086\n",
      "Accuracy: 97.88%\n",
      "Train time: 17.0 minutes\n",
      "Run end --------------------------\n",
      "\n",
      "Run start ------------------------\n",
      "run:\t3\n",
      "n_hl:\t1\n",
      "n_hu:\t268\n",
      "rate:\t1\n",
      "lmda:\t0.0001\n",
      "act:\trelu\n",
      "Cost after iteration 0: 2.301997\n",
      "Cost after iteration 400: 0.105562\n",
      "Cost after iteration 800: 0.059944\n",
      "Cost after iteration 1200: 0.039559\n",
      "Cost after iteration 1600: 0.027739\n",
      "Final cost: 0.020218\n",
      "Accuracy: 97.89%\n",
      "Train time: 25.0 minutes\n",
      "Run end --------------------------\n",
      "\n",
      "Run start ------------------------\n",
      "run:\t4\n",
      "n_hl:\t1\n",
      "n_hu:\t354\n",
      "rate:\t1\n",
      "lmda:\t0.0001\n",
      "act:\trelu\n",
      "Cost after iteration 0: 2.302907\n",
      "Cost after iteration 400: 0.106645\n",
      "Cost after iteration 800: 0.060561\n",
      "Cost after iteration 1200: 0.040125\n",
      "Cost after iteration 1600: 0.028353\n",
      "Final cost: 0.020814\n",
      "Accuracy: 97.95%\n",
      "Train time: 34.0 minutes\n",
      "Run end --------------------------\n",
      "\n",
      "Run start ------------------------\n",
      "run:\t5\n",
      "n_hl:\t1\n",
      "n_hu:\t440\n",
      "rate:\t1\n",
      "lmda:\t0.0001\n",
      "act:\trelu\n",
      "Cost after iteration 0: 2.303358\n",
      "Cost after iteration 400: 0.099596\n",
      "Cost after iteration 800: 0.054843\n",
      "Cost after iteration 1200: 0.035588\n",
      "Cost after iteration 1600: 0.024613\n",
      "Final cost: 0.017756\n",
      "Accuracy: 97.97%\n",
      "Train time: 40.0 minutes\n",
      "Run end --------------------------\n",
      "\n",
      "Best run -------------------------\n",
      "run:\t5\n",
      "n_hl:\t1\n",
      "n_hu:\t440\n",
      "rate:\t1\n",
      "lmda:\t0.0001\n",
      "act:\trelu\n",
      "acc:\t97.97\n",
      "params:\t{}\n",
      "Run end --------------------------\n"
     ]
    }
   ],
   "source": [
    "num_iterations = 2000 # Increase iterations.\n",
    "layer_list = [1] # Keep fixed (best performing).\n",
    "unit_list = [96, 182, 268, 354, 440] # Try 1/6th increments between 10 and 526.\n",
    "rate_list = [1] # Keep fixed.\n",
    "lmda_list = [0.0001] # Keep fixed.\n",
    "act_list = ['relu'] # Keep fixed (best performing).\n",
    "best_run, results = parameter_tuning(layer_list, unit_list, rate_list, lmda_list, act_list, num_iterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best performing run had n_hu = 440. However, this is a very small accuracy increase from n_hu = 182, which took half the time to train. Because of this, I chose n_hu = 182, n_hl = 1 for my NN architecture.\n",
    "\n",
    "I tried to tune learning rate and regularization in order to reduce any overfitting that might be occurring. With fast training times on this NN architecture, I also increased num_iterations to see if performance would be improved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Run start ------------------------\n",
      "run:\t1\n",
      "n_hl:\t1\n",
      "n_hu:\t182\n",
      "rate:\t1\n",
      "lmda:\t0.001\n",
      "act:\trelu\n",
      "Cost after iteration 0: 2.303443\n",
      "Cost after iteration 800: 0.060234\n",
      "Cost after iteration 1600: 0.027539\n",
      "Cost after iteration 2400: 0.015121\n",
      "Cost after iteration 3200: 0.009321\n",
      "Final cost: 0.006277\n",
      "Accuracy: 98.05%\n",
      "Train time: 50.0 minutes\n",
      "Run end --------------------------\n",
      "\n",
      "Run start ------------------------\n",
      "run:\t2\n",
      "n_hl:\t1\n",
      "n_hu:\t182\n",
      "rate:\t1\n",
      "lmda:\t0.01\n",
      "act:\trelu\n",
      "Cost after iteration 0: 2.303444\n",
      "Cost after iteration 800: 0.060258\n",
      "Cost after iteration 1600: 0.027589\n",
      "Cost after iteration 2400: 0.015174\n",
      "Cost after iteration 3200: 0.009377\n",
      "Final cost: 0.006333\n",
      "Accuracy: 98.06%\n",
      "Train time: 50.0 minutes\n",
      "Run end --------------------------\n",
      "\n",
      "Run start ------------------------\n",
      "run:\t3\n",
      "n_hl:\t1\n",
      "n_hu:\t182\n",
      "rate:\t1\n",
      "lmda:\t0.1\n",
      "act:\trelu\n",
      "Cost after iteration 0: 2.303455\n",
      "Cost after iteration 800: 0.060623\n",
      "Cost after iteration 1600: 0.027906\n",
      "Cost after iteration 2400: 0.015594\n",
      "Cost after iteration 3200: 0.009900\n",
      "Final cost: 0.006926\n",
      "Accuracy: 98.07000000000001%\n",
      "Train time: 46.0 minutes\n",
      "Run end --------------------------\n",
      "\n",
      "Run start ------------------------\n",
      "run:\t4\n",
      "n_hl:\t1\n",
      "n_hu:\t182\n",
      "rate:\t0.75\n",
      "lmda:\t0.001\n",
      "act:\trelu\n",
      "Cost after iteration 0: 2.303443\n",
      "Cost after iteration 800: 0.075778\n",
      "Cost after iteration 1600: 0.037149\n",
      "Cost after iteration 2400: 0.021653\n",
      "Cost after iteration 3200: 0.013869\n",
      "Final cost: 0.009545\n",
      "Accuracy: 98.11%\n",
      "Train time: 41.0 minutes\n",
      "Run end --------------------------\n",
      "\n",
      "Run start ------------------------\n",
      "run:\t5\n",
      "n_hl:\t1\n",
      "n_hu:\t182\n",
      "rate:\t0.75\n",
      "lmda:\t0.01\n",
      "act:\trelu\n",
      "Cost after iteration 0: 2.303444\n",
      "Cost after iteration 800: 0.075823\n",
      "Cost after iteration 1600: 0.037193\n",
      "Cost after iteration 2400: 0.021699\n",
      "Cost after iteration 3200: 0.013930\n",
      "Final cost: 0.009602\n",
      "Accuracy: 98.1%\n",
      "Train time: 40.0 minutes\n",
      "Run end --------------------------\n",
      "\n",
      "Run start ------------------------\n",
      "run:\t6\n",
      "n_hl:\t1\n",
      "n_hu:\t182\n",
      "rate:\t0.75\n",
      "lmda:\t0.1\n",
      "act:\trelu\n",
      "Cost after iteration 0: 2.303455\n",
      "Cost after iteration 800: 0.075964\n",
      "Cost after iteration 1600: 0.037472\n",
      "Cost after iteration 2400: 0.022055\n",
      "Cost after iteration 3200: 0.014324\n",
      "Final cost: 0.010043\n",
      "Accuracy: 98.11%\n",
      "Train time: 33.0 minutes\n",
      "Run end --------------------------\n",
      "\n",
      "Run start ------------------------\n",
      "run:\t7\n",
      "n_hl:\t1\n",
      "n_hu:\t182\n",
      "rate:\t0.5\n",
      "lmda:\t0.001\n",
      "act:\trelu\n",
      "Cost after iteration 0: 2.303443\n",
      "Cost after iteration 800: 0.111460\n",
      "Cost after iteration 1600: 0.059446\n",
      "Cost after iteration 2400: 0.037957\n",
      "Cost after iteration 3200: 0.026153\n",
      "Final cost: 0.018847\n",
      "Accuracy: 97.94%\n",
      "Train time: 34.0 minutes\n",
      "Run end --------------------------\n",
      "\n",
      "Run start ------------------------\n",
      "run:\t8\n",
      "n_hl:\t1\n",
      "n_hu:\t182\n",
      "rate:\t0.5\n",
      "lmda:\t0.01\n",
      "act:\trelu\n",
      "Cost after iteration 0: 2.303444\n",
      "Cost after iteration 800: 0.111435\n",
      "Cost after iteration 1600: 0.059439\n",
      "Cost after iteration 2400: 0.037965\n",
      "Cost after iteration 3200: 0.026180\n",
      "Final cost: 0.018894\n",
      "Accuracy: 97.95%\n",
      "Train time: 34.0 minutes\n",
      "Run end --------------------------\n",
      "\n",
      "Run start ------------------------\n",
      "run:\t9\n",
      "n_hl:\t1\n",
      "n_hu:\t182\n",
      "rate:\t0.5\n",
      "lmda:\t0.1\n",
      "act:\trelu\n",
      "Cost after iteration 0: 2.303455\n",
      "Cost after iteration 800: 0.111713\n",
      "Cost after iteration 1600: 0.059717\n",
      "Cost after iteration 2400: 0.038312\n",
      "Cost after iteration 3200: 0.026539\n",
      "Final cost: 0.019273\n",
      "Accuracy: 97.95%\n",
      "Train time: 34.0 minutes\n",
      "Run end --------------------------\n",
      "\n",
      "Best run -------------------------\n",
      "run:\t4\n",
      "n_hl:\t1\n",
      "n_hu:\t182\n",
      "rate:\t0.5\n",
      "lmda:\t0.01\n",
      "act:\trelu\n",
      "acc:    98.11\n",
      "params:\t{}\n",
      "Run end --------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_iterations = 4000 # Increase iterations.\n",
    "layer_list = [1] # Keep fixed (best performance).\n",
    "unit_list = [182] # Keep fixed (best performance).\n",
    "rate_list = [1, 0.75, 0.5] # Try different learning rates (0.25 increments).\n",
    "lmda_list = [0.001, 0.01, 0.1] # Try different lambdas (logarithmic increments).\n",
    "act_list = ['relu'] # Keep fixed (best performing).\n",
    "best_run, results = parameter_tuning(layer_list, unit_list, rate_list, lmda_list, act_list, num_iterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best run had slightly improved accuracy in comparison to learning_rate = 1 and lambda = 0.0001. Therefore, I chose lambda = 0.01 and learning_rate = 0.5, along with n_hl = 1 and n_hu = 182, as my final NN architecture.\n",
    "\n",
    "I tried one last run of my chosen NN architecture with greatly increased training iterations, to see if there was any performance to be gained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run start ------------------------\n",
      "run:\t1\n",
      "n_hl:\t1\n",
      "n_hu:\t182\n",
      "rate:\t0.5\n",
      "lmda:\t0.01\n",
      "act:\trelu\n",
      "Cost after iteration 0: 2.303444\n",
      "Cost after iteration 1600: 0.059439\n",
      "Cost after iteration 3200: 0.026180\n",
      "Cost after iteration 4800: 0.014099\n",
      "Cost after iteration 6400: 0.008571\n",
      "Final cost: 0.005754\n",
      "Accuracy: 98.13%\n",
      "Train time: 67.0 minutes\n",
      "Run end --------------------------\n",
      "\n",
      "Best run -------------------------\n",
      "run:\t1\n",
      "n_hl:\t1\n",
      "n_hu:\t182\n",
      "rate:\t0.5\n",
      "lmda:\t0.01\n",
      "act:\trelu\n",
      "acc:\t98.13\n",
      "params:\t{}\n",
      "Run end --------------------------\n"
     ]
    }
   ],
   "source": [
    "num_iterations = 8000 # Increase iterations.\n",
    "# All parameters below deemed best perfoming.\n",
    "layer_list = [1]\n",
    "unit_list = [182]\n",
    "rate_list = [0.5]\n",
    "lmda_list = [0.01]\n",
    "act_list = ['relu']\n",
    "best_run, results = parameter_tuning(layer_list, unit_list, rate_list, lmda_list, act_list, num_iterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best accuracy I could achieve was 98.13%, given training time constraints. I believe the best NN architectures manage to get ~99%. In conclusion, a standard FFNN does pretty well with the MNIST dataset."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "schipper_as3",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
